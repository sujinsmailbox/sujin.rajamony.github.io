# <div align="center"> Sujin Rajamony  
**<div align="center"> Big Data Architect-AWS**  
sujinsmailbox@gmail.com | +1 647-393-6725 | [linkedin.com/in/rsujin/](https://linkedin.com/in/rsujin/)  
</div>

Full-Stack Data Engineer with 13+ years of experience, responsible for designing and managing end-to-end data pipelines, platform provisioning, and automation. Skilled in ingestion, transformation, governance, analytics, and CI/CD. Bootstrapped and optimized data engineering infrastructure, delivering scalable solutions and staying ahead of emerging technologies. 



---
<h4 style="padding-left: 60px;">Data Engineering Skills</h4>

<div style="padding-left: 60px;"> 

| **Big Data Platforms**               | **Data Engineering Skills**            |
|:-------------------------------------------|----------------------------------------|
| AWS Cloud                                | AWS Data Lake ( S3, Lakeformation, DMS, Kinesis, Glue, Redshift)                     |
| Apache Spark                              | Data Governance (Cataloging, Lineage, Quality Check, Access) |
| Apache Airflow                               |Data Warehousing (Modeling and Optimization)  |
| Redshift                                 | Data Modeling (Dimensional and ER Models) |
|BigQuery                                | Data Pipelines (ETL/ELT Processes)     |
| Google Analytics                         | DBT (Data Build Tool for Transformations) |

</div>

<h4 style="padding-left: 60px;">Programing and Devops</h4>
<div style="padding-left: 60px;">

| **Programming Skills**   |**DevOps & Automation Skills**                           |
|-------------------------------------------|----------------------------------------|
| Python (Data Engineering, APIs, Automation)  |GitHub Pipelines (CI/CD Automation) |
|C# (Object-Oriented Programming, ASP.Net) |  AWS CDK (Infrastructure as Code)          |

</div>

---

## Experience  

### **Lead Data Engineer**  
**Einc** (May 2023 – Present) 
#### Data Lake and Platform Setup  
- Designed and implemented a data lake platform on AWS S3 using Lake Formation, Glue, Redshift Spectrum, and Redshift Materialized Views, enabling efficient data storage and analytics consumption.  
- Configured Redshift for advanced capabilities, including federated queries to access RDS, data sharing between production and development accounts, external data sharing, and real-time data ingestion using Kinesis.  

#### Data Ingestion and Integration  
- Built a scalable data ingestion solution to ingest data from multiple platforms into the AWS Data Lake.  
- Ingested Google Analytics data from GCP into AWS, enabling seamless access and analysis for downstream users.  
- Developed Glue jobs for ingesting data from external tools like HubSpot, Zoho CRM, Zoho Desk, Zendesk, Dayforce, and Google Analytics.  
- Captured change data (CDC) using DMS and Kinesis, routing it into Redshift Materialized Views for transformation and analytics.  

#### Data Transformation and Analytics  
- Leveraged **DBT (Data Build Tool)** to transform ingested data into fact and dimension tables for efficient reporting and analytics.  
- Implemented DBT workflows to standardize and optimize data pipelines, scheduled and orchestrated using Airflow.  

#### Data Pipeline Orchestration  
- Orchestrated pipelines using MWAA (Managed Workflows for Apache Airflow), Fivetran, and Glue Jobs, with AWS S3 Data Lake and Redshift as destinations.  
- Built end-to-end automation for ingestion, transformation, and reporting, ensuring robust and scalable pipelines.  

#### Observability and Monitoring  
- Developed observability solutions using CloudWatch logs and metrics, setting up EventBridge rules to trigger SNS notifications in Slack for real-time monitoring.  
- Proactively resolved pipeline issues by monitoring SLA breaches and failures.  

#### Infrastructure as Code and CI/CD  
- Implemented Infrastructure as Code (IaC) for all solutions using AWS CDK, ensuring reproducible and scalable infrastructure deployments.  
- Automated testing, deployment, and versioning with CI/CD pipelines using GitHub Action Workflows.  

#### Documentation and Collaboration  
- Documented architecture, workflows, and processes on Confluence, enabling centralized knowledge sharing and onboarding.  
- Tracked and prioritized development tasks using Jira to ensure timely delivery and efficient project execution.  
- Collaborated with the Data Science team to ensure data readiness for ML models and analytics. Partnered with the Reporting team to deliver optimized datasets for BI dashboards. Regularly aligned with stakeholders to meet evolving business requirements.  

---

### **Senior Data Engineer**  
**GoDaddy** (May 2021 – May 2023)  
- Spearheaded data migration projects, creating data models, lineage, and design documents for datasets ingested into the AWS data lake.  
- Designed a centralized data lake to enable a data mesh architecture using Lake Formation for secure cross-account access across AWS.  
- Developed a reusable DAG Factory framework in Apache Airflow using Python for seamless integration with AWS services and efficient orchestration.  
- Optimized EMR cluster functionality and latency in ingestion pipelines using Airflow.  
- Built CI/CD pipelines using GitHub Actions with self-hosted runners, automating deployment and testing processes.  
- Processed raw datasets into analytical datasets in Redshift Spectrum by applying complex transformations and data cleansing techniques.  
- Led data quality initiatives by implementing SLAs and failure detection mechanisms across marketing and ad pipelines.  

---

### **Big Data Engineer**  
**Emis Health Private Ltd - India** (Nov 2018 – Mar 2021)  
- Played a critical role in planning and developing a robust Data Lake architecture during the migration of on-premises infrastructure to the cloud.  
- Developed scalable Python packages for ingesting diverse data formats (CSV, JSON, XML, Parquet) into the AWS environment.  
- Automated data extractions with dynamic Airflow DAGs and securely delivered processed data via SFTP.  
- Optimized and processed large-scale datasets with complex schemas using SQL and EMR Spark.  
- Integrated Spark with Databricks Delta Lake for ACID compliance and improved data retrieval speeds with partition pruning techniques.  
- Managed AWS cloud infrastructure, including S3, Glue, EC2, IAM, KMS, Route53, ECR, CodePipeline, and EMR clusters with Hadoop/Yarn/Hive.  
- Prototyped real-time data ingestion using Python, Spark Structured Streaming, and Kafka for efficient streaming data processing.  

---

### **Data Engineer**  
**Qburst Technologies & Emis Health Private Ltd** (Sep 2015 – Sep 2018)  
- Developed SSIS packages for ETL operations from multiple source databases into a Microsoft Parallel Data Warehouse.  
- Reduced extract job runtimes from over 24 hours to under 10 hours by optimizing queries in SQL Server and Parallel Data Warehouse.  
- Designed and implemented efficient data warehouse models to improve processing during migration.  
- Eliminated frequent patching needs by creating a version-independent reverse logging system.  
- Developed a live data extraction tool in .NET, automating extraction from multiple databases and generating daily customer reports.  

---

### **Application Developer**  
**Cognizant Technology Solutions, Chennai, Tamil Nadu** (Jun 2011 – Sep 2015)  
- Developed and maintained custom web applications using ASP.NET, SQL Server, JQuery, HTML, C#, and Web API.  
- Designed a new user interface for reporting systems with advanced filtering options, benefiting multi-dimensional reporting.  

---

## Education  
**Bachelor of Technology: Information Technology** (Sep 2006 - May 2010)  
St Xavier’s Catholic College of Engineering, Tamil Nadu, India  

---

### Achievements  
- Recognized and appreciated multiple times for fixing performance issues on live data warehousing systems, traveling to the UK four times to work onsite with experts.  
- Awarded Star Performer several times for sustained excellent performance in carrying out the deliverables with dedication and enthusiasm.
